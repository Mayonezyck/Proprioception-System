# deformable_explore
This Repository is for me to work at the robot both at home and at work.
There will be a mix of projects in this repo, but the entire point is to have this repository be a box for all the tools that I made so far about the deformable object mainpulation project. 

The repo will be structured as a big tree, and each branch will be found containing different kinds of tools. The structure that I have in mind now is: camera, robots, ROS, and other tools.

## Introduction to the project, and maybe just some thought from me.
If we think of robots as a form of life that we create, we use human beings (ourselves) as a template. I love to see this work in this way. Like look, we are literally trying to carry things from human a bit and a bit to this metal artifact called robots. The logic is intuitive. We are most intuitively transferring working knowledge into the unknown fields, trying to see the transferable success. 

It is believed that robots need perception to work with this physical world. We as human beings are not able to imaging ways to interact with the world without perceptions. Different senses, or modalities combine different information about the world. With only vision, one cannot so easily learn about the surface texture; without smell, the perception of taste is also impaired. 

As the development of robotics progress, the increasing need for automation brings more and more sensors onto robots. In the below section I am going to talk about different sensors on robots and their jobs. In another section, I will be talking about how those modalities are fused together and shaped to the form of the 'world' in the 'mind' of robots.

### Human perception comparing to robot perception
+ Vision: the ability to see, is a sense that's shared amongst a variety of living things on earth. Vision is the sense for the incoming light. A healthy human is equipped with stereo binocular 'cameras', the two eyes that we have. The binocular design gives us a wider field of view, and the perception of depth in the overlapping area. The perception of depth is achieved by recognizing the distance between one feature point in left and right images: if the feature point is far away, the distance between the eye and the feature point is close; on the other hand, if the distance is close, the distance between is far. Without two eyes fully open, our perception of depth got impaired. However, even with one eye closed, some sense of depth still remains. Actually, we can move around and see the difference of moving distance. What if the scene is stationary and monocular? We still can capture the depth information because how it looks like from our experience of the world, for example, looking at a picture we can tell the mountain is behind the buildings. Although it is harder to get a correct answer when we are eliminating the inputs, it is not impossible to do so. Robots can be equipped with a larger variety of visual inputs, enabling the sensing in larger range comparing to visible light. Lidars are also widely used on robots nowadays, which use light pulses to sense distance. 

+ Audition/Sound

+ Touch/Somatosensation
Human has a very delicate organ, skin, as the sensor of touch. A huge network of nerve endings and touch receptors are distributed around in the skin, that returns all the touch sensation, including temperature, friction, pressure, tickle, pain, vibrations, etc. [[1]](1) In robots

+ Taste/Gustation
+ Smell/Olfaction
+ Balance/Vestibular Sense
+ Proprioception


# Citation
<a id="1">[1]</a> : Groove, “Sense of Touch, Skin Receptors, Skin Sensations, Somatosensory System,” Home Science Tools Resource Center. Accessed: Mar. 10, 2025. [Online]. Available: https://learning-center.homesciencetools.com/article/skin-touch/
