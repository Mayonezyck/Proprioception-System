{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0UntaQv3BtP"
   },
   "source": [
    "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/img/realsense.png\" width=\"70%\" /></p>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The notebook offers a quick hands-on introduction to Intel RealSense Depth-Sensing technology. \n",
    "\n",
    "> **Have a Question?** [Open new issue on our GitHub](https://github.com/IntelRealSense/librealsense/issues/new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkVvgf-z4D1f"
   },
   "source": [
    "## The Tools\n",
    "We are planning to use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y2jplwqU4OHe",
    "outputId": "7a1685ad-c7ed-4fb9-9d76-eb728609f97f"
   },
   "outputs": [],
   "source": [
    "import cv2                                # state of the art computer vision algorithms library\n",
    "import numpy as np                        # fundamental package for scientific computing\n",
    "import matplotlib.pyplot as plt           # 2D plotting library producing publication quality figures\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "print(\"Environment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwiY9raM5nCH"
   },
   "source": [
    "## The API\n",
    "Next, we will open depth and RGB streams from pre-recorded file and capture a set of frames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cV1OkBYn55VH",
    "outputId": "0fbae060-91a8-4f2f-acf8-c20a0cf33dcd"
   },
   "outputs": [],
   "source": [
    "# Setup:\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "# cfg.enable_device_from_file(\"../object_detection.bag\")\n",
    "profile = pipe.start(cfg)\n",
    "\n",
    "# Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "for x in range(5):\n",
    "  pipe.wait_for_frames()\n",
    "  \n",
    "# Store next frameset for later processing:\n",
    "frameset = pipe.wait_for_frames()\n",
    "color_frame = frameset.get_color_frame()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "# Cleanup:\n",
    "pipe.stop()\n",
    "print(\"Frames Captured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxlrWpLj6mkJ"
   },
   "source": [
    "## RGB Data\n",
    "Let's start with accessing the color componnent of the frameset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "5ajjn7Wq6nEg",
    "outputId": "737674f4-5314-470a-99c0-662f35444e02"
   },
   "outputs": [],
   "source": [
    "color = np.asanyarray(color_frame.get_data())\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.imshow(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2JsSsF47uZC"
   },
   "source": [
    "## Depth Data\n",
    "Now, we will visualize the depth map captured by the RealSense camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "2hF4VSna-6kR",
    "outputId": "d6e9c568-76b9-4b4f-a978-ea91f0c924f2"
   },
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPnn1vivBJgC"
   },
   "source": [
    "## Stream Alignment\n",
    "Upon closer inspection you can notice that the two frames are not captured from the same physical viewport.\n",
    "\n",
    "To combine them into a single RGBD image, let's align depth data to color viewport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzPfow5oBhyj"
   },
   "outputs": [],
   "source": [
    "# Create alignment primitive with color as its target stream:\n",
    "align = rs.align(rs.stream.color)\n",
    "frameset = align.process(frameset)\n",
    "\n",
    "# Update color and depth frames:\n",
    "aligned_depth_frame = frameset.get_depth_frame()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "# Show the two frames together:\n",
    "images = np.hstack((color, colorized_depth))\n",
    "plt.imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDPAZ2wvCg50"
   },
   "source": [
    "Now the two images are pixel-perfect aligned and you can use depth data just like you would any of the other channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JM4Fz0vhoj0"
   },
   "source": [
    "## Object Detection\n",
    "\n",
    "Next, we will take advantage of widely popular **MobileNet SSD Model** to recognize and localize objects in the scene and use additional depth data to enrich our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "z4sPyGkWY_co",
    "outputId": "5d6dd74a-4986-4c63-deee-cf1cc95610f8"
   },
   "outputs": [],
   "source": [
    "# Standard OpenCV boilerplate for running the net:\n",
    "height, width = color.shape[:2]\n",
    "expected = 300\n",
    "aspect = width / height\n",
    "resized_image = cv2.resize(color, (round(expected * aspect), expected))\n",
    "crop_start = round(expected * (aspect - 1) / 2)\n",
    "crop_img = resized_image[0:expected, crop_start:crop_start+expected]\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(\"../MobileNetSSD_deploy.prototxt\", \"../MobileNetSSD_deploy.caffemodel\")\n",
    "inScaleFactor = 0.007843\n",
    "meanVal       = 127.53\n",
    "classNames = (\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "              \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
    "              \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "              \"motorbike\", \"person\", \"pottedplant\",\n",
    "              \"sheep\", \"sofa\", \"train\", \"tvmonitor\")\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(crop_img, inScaleFactor, (expected, expected), meanVal, False)\n",
    "net.setInput(blob, \"data\")\n",
    "detections = net.forward(\"detection_out\")\n",
    "\n",
    "label = detections[0,0,0,1]\n",
    "conf  = detections[0,0,0,2]\n",
    "xmin  = detections[0,0,0,3]\n",
    "ymin  = detections[0,0,0,4]\n",
    "xmax  = detections[0,0,0,5]\n",
    "ymax  = detections[0,0,0,6]\n",
    "\n",
    "className = classNames[int(label)]\n",
    "\n",
    "cv2.rectangle(crop_img, (int(xmin * expected), int(ymin * expected)), \n",
    "             (int(xmax * expected), int(ymax * expected)), (255, 255, 255), 2)\n",
    "cv2.putText(crop_img, className, \n",
    "            (int(xmin * expected), int(ymin * expected) - 5),\n",
    "            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255))\n",
    "\n",
    "plt.imshow(crop_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyECTYucnjB_"
   },
   "source": [
    "By projecting this data into the depth channel, we can now answer additional questions we couldn't approach before.\n",
    "\n",
    "For example, with computer vision only it would be rather hard to make any meaningful predictions about **size and distance**.\n",
    "You could train a model on average dog size per breed, but it would be easily fooled by toys of dogs or dogs of irregular proportions. Instead you can get this information directly when you have depth available!\n",
    "\n",
    "Let's project our detected bounding box on to the depth image, and average the depth data inside it to get a sense of how close is the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "l3fW-u7mlo3V",
    "outputId": "5904a9ae-bed7-4bf8-ce2d-144750851b07"
   },
   "outputs": [],
   "source": [
    "scale = height / expected\n",
    "xmin_depth = int((xmin * expected + crop_start) * scale)\n",
    "ymin_depth = int((ymin * expected) * scale)\n",
    "xmax_depth = int((xmax * expected + crop_start) * scale)\n",
    "ymax_depth = int((ymax * expected) * scale)\n",
    "xmin_depth,ymin_depth,xmax_depth,ymax_depth\n",
    "cv2.rectangle(colorized_depth, (xmin_depth, ymin_depth), \n",
    "             (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cz-mgKk7l-xm",
    "outputId": "08b3f153-ad66-4d28-c58d-de24064d5f8f"
   },
   "outputs": [],
   "source": [
    "depth = np.asanyarray(aligned_depth_frame.get_data())\n",
    "# Crop depth data:\n",
    "depth = depth[xmin_depth:xmax_depth,ymin_depth:ymax_depth].astype(float)\n",
    "\n",
    "# Get data scale from the device and convert to meters\n",
    "depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "depth = depth * depth_scale\n",
    "dist,_,_,_ = cv2.mean(depth)\n",
    "print(\"Detected a {0} {1:.3} meters away.\".format(className, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4TPljtWrUlw"
   },
   "source": [
    "This is a rather simple example, but it gives you a taste of what can be accomplished by combining depth with modern computer vision.\n",
    "\n",
    "> **Want to learn more?** Visit [realsense.intel.com](http://realsense.intel.com) and [github.com/IntelRealSense](http://www.github.com/IntelRealSense/librealsense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: Apache 2.0. See LICENSE file in root directory.\n",
    "# Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "\"\"\"\n",
    "OpenCV and Numpy Point cloud Software Renderer\n",
    "\n",
    "This sample is mostly for demonstration and educational purposes.\n",
    "It really doesn't offer the quality or performance that can be\n",
    "achieved with hardware acceleration.\n",
    "\n",
    "Usage:\n",
    "------\n",
    "Mouse: \n",
    "    Drag with left button to rotate around pivot (thick small axes), \n",
    "    with right button to translate and the wheel to zoom.\n",
    "\n",
    "Keyboard: \n",
    "    [p]     Pause\n",
    "    [r]     Reset View\n",
    "    [d]     Cycle through decimation values\n",
    "    [z]     Toggle point scaling\n",
    "    [c]     Toggle color source\n",
    "    [s]     Save PNG (./out.png)\n",
    "    [e]     Export points to ply (./out.ply)\n",
    "    [q\\ESC] Quit\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "class AppState:\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.WIN_NAME = 'RealSense'\n",
    "        self.pitch, self.yaw = math.radians(-10), math.radians(-15)\n",
    "        self.translation = np.array([0, 0, -1], dtype=np.float32)\n",
    "        self.distance = 2\n",
    "        self.prev_mouse = 0, 0\n",
    "        self.mouse_btns = [False, False, False]\n",
    "        self.paused = False\n",
    "        self.decimate = 1\n",
    "        self.scale = True\n",
    "        self.color = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.pitch, self.yaw, self.distance = 0, 0, 2\n",
    "        self.translation[:] = 0, 0, -1\n",
    "\n",
    "    @property\n",
    "    def rotation(self):\n",
    "        Rx, _ = cv2.Rodrigues((self.pitch, 0, 0))\n",
    "        Ry, _ = cv2.Rodrigues((0, self.yaw, 0))\n",
    "        return np.dot(Ry, Rx).astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def pivot(self):\n",
    "        return self.translation + np.array((0, 0, self.distance), dtype=np.float32)\n",
    "\n",
    "\n",
    "state = AppState()\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "# Get stream profile and camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "\n",
    "# Processing blocks\n",
    "pc = rs.pointcloud()\n",
    "decimate = rs.decimation_filter()\n",
    "decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)\n",
    "colorizer = rs.colorizer()\n",
    "\n",
    "\n",
    "def mouse_cb(event, x, y, flags, param):\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        state.mouse_btns[0] = True\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONUP:\n",
    "        state.mouse_btns[0] = False\n",
    "\n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        state.mouse_btns[1] = True\n",
    "\n",
    "    if event == cv2.EVENT_RBUTTONUP:\n",
    "        state.mouse_btns[1] = False\n",
    "\n",
    "    if event == cv2.EVENT_MBUTTONDOWN:\n",
    "        state.mouse_btns[2] = True\n",
    "\n",
    "    if event == cv2.EVENT_MBUTTONUP:\n",
    "        state.mouse_btns[2] = False\n",
    "\n",
    "    if event == cv2.EVENT_MOUSEMOVE:\n",
    "\n",
    "        h, w = out.shape[:2]\n",
    "        dx, dy = x - state.prev_mouse[0], y - state.prev_mouse[1]\n",
    "\n",
    "        if state.mouse_btns[0]:\n",
    "            state.yaw += float(dx) / w * 2\n",
    "            state.pitch -= float(dy) / h * 2\n",
    "\n",
    "        elif state.mouse_btns[1]:\n",
    "            dp = np.array((dx / w, dy / h, 0), dtype=np.float32)\n",
    "            state.translation -= np.dot(state.rotation, dp)\n",
    "\n",
    "        elif state.mouse_btns[2]:\n",
    "            dz = math.sqrt(dx**2 + dy**2) * math.copysign(0.01, -dy)\n",
    "            state.translation[2] += dz\n",
    "            state.distance -= dz\n",
    "\n",
    "    if event == cv2.EVENT_MOUSEWHEEL:\n",
    "        dz = math.copysign(0.1, flags)\n",
    "        state.translation[2] += dz\n",
    "        state.distance -= dz\n",
    "\n",
    "    state.prev_mouse = (x, y)\n",
    "\n",
    "\n",
    "cv2.namedWindow(state.WIN_NAME, cv2.WINDOW_AUTOSIZE)\n",
    "cv2.resizeWindow(state.WIN_NAME, w, h)\n",
    "cv2.setMouseCallback(state.WIN_NAME, mouse_cb)\n",
    "\n",
    "\n",
    "def project(v):\n",
    "    \"\"\"project 3d vector array to 2d\"\"\"\n",
    "    h, w = out.shape[:2]\n",
    "    view_aspect = float(h)/w\n",
    "\n",
    "    # ignore divide by zero for invalid depth\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        proj = v[:, :-1] / v[:, -1, np.newaxis] * \\\n",
    "            (w*view_aspect, h) + (w/2.0, h/2.0)\n",
    "\n",
    "    # near clipping\n",
    "    znear = 0.03\n",
    "    proj[v[:, 2] < znear] = np.nan\n",
    "    return proj\n",
    "\n",
    "\n",
    "def view(v):\n",
    "    \"\"\"apply view transformation on vector array\"\"\"\n",
    "    return np.dot(v - state.pivot, state.rotation) + state.pivot - state.translation\n",
    "\n",
    "\n",
    "def line3d(out, pt1, pt2, color=(0x80, 0x80, 0x80), thickness=1):\n",
    "    \"\"\"draw a 3d line from pt1 to pt2\"\"\"\n",
    "    p0 = project(pt1.reshape(-1, 3))[0]\n",
    "    p1 = project(pt2.reshape(-1, 3))[0]\n",
    "    if np.isnan(p0).any() or np.isnan(p1).any():\n",
    "        return\n",
    "    p0 = tuple(p0.astype(int))\n",
    "    p1 = tuple(p1.astype(int))\n",
    "    rect = (0, 0, out.shape[1], out.shape[0])\n",
    "    inside, p0, p1 = cv2.clipLine(rect, p0, p1)\n",
    "    if inside:\n",
    "        cv2.line(out, p0, p1, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def grid(out, pos, rotation=np.eye(3), size=1, n=10, color=(0x80, 0x80, 0x80)):\n",
    "    \"\"\"draw a grid on xz plane\"\"\"\n",
    "    pos = np.array(pos)\n",
    "    s = size / float(n)\n",
    "    s2 = 0.5 * size\n",
    "    for i in range(0, n+1):\n",
    "        x = -s2 + i*s\n",
    "        line3d(out, view(pos + np.dot((x, 0, -s2), rotation)),\n",
    "               view(pos + np.dot((x, 0, s2), rotation)), color)\n",
    "    for i in range(0, n+1):\n",
    "        z = -s2 + i*s\n",
    "        line3d(out, view(pos + np.dot((-s2, 0, z), rotation)),\n",
    "               view(pos + np.dot((s2, 0, z), rotation)), color)\n",
    "\n",
    "\n",
    "def axes(out, pos, rotation=np.eye(3), size=0.075, thickness=2):\n",
    "    \"\"\"draw 3d axes\"\"\"\n",
    "    line3d(out, pos, pos +\n",
    "           np.dot((0, 0, size), rotation), (0xff, 0, 0), thickness)\n",
    "    line3d(out, pos, pos +\n",
    "           np.dot((0, size, 0), rotation), (0, 0xff, 0), thickness)\n",
    "    line3d(out, pos, pos +\n",
    "           np.dot((size, 0, 0), rotation), (0, 0, 0xff), thickness)\n",
    "\n",
    "\n",
    "def frustum(out, intrinsics, color=(0x40, 0x40, 0x40)):\n",
    "    \"\"\"draw camera's frustum\"\"\"\n",
    "    orig = view([0, 0, 0])\n",
    "    w, h = intrinsics.width, intrinsics.height\n",
    "\n",
    "    for d in range(1, 6, 2):\n",
    "        def get_point(x, y):\n",
    "            p = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], d)\n",
    "            line3d(out, orig, view(p), color)\n",
    "            return p\n",
    "\n",
    "        top_left = get_point(0, 0)\n",
    "        top_right = get_point(w, 0)\n",
    "        bottom_right = get_point(w, h)\n",
    "        bottom_left = get_point(0, h)\n",
    "\n",
    "        line3d(out, view(top_left), view(top_right), color)\n",
    "        line3d(out, view(top_right), view(bottom_right), color)\n",
    "        line3d(out, view(bottom_right), view(bottom_left), color)\n",
    "        line3d(out, view(bottom_left), view(top_left), color)\n",
    "\n",
    "\n",
    "def pointcloud(out, verts, texcoords, color, painter=True):\n",
    "    \"\"\"draw point cloud with optional painter's algorithm\"\"\"\n",
    "    if painter:\n",
    "        # Painter's algo, sort points from back to front\n",
    "\n",
    "        # get reverse sorted indices by z (in view-space)\n",
    "        # https://gist.github.com/stevenvo/e3dad127598842459b68\n",
    "        v = view(verts)\n",
    "        s = v[:, 2].argsort()[::-1]\n",
    "        proj = project(v[s])\n",
    "    else:\n",
    "        proj = project(view(verts))\n",
    "\n",
    "    if state.scale:\n",
    "        proj *= 0.5**state.decimate\n",
    "\n",
    "    h, w = out.shape[:2]\n",
    "\n",
    "    # proj now contains 2d image coordinates\n",
    "    j, i = proj.astype(np.uint32).T\n",
    "\n",
    "    # create a mask to ignore out-of-bound indices\n",
    "    im = (i >= 0) & (i < h)\n",
    "    jm = (j >= 0) & (j < w)\n",
    "    m = im & jm\n",
    "\n",
    "    cw, ch = color.shape[:2][::-1]\n",
    "    if painter:\n",
    "        # sort texcoord with same indices as above\n",
    "        # texcoords are [0..1] and relative to top-left pixel corner,\n",
    "        # multiply by size and add 0.5 to center\n",
    "        v, u = (texcoords[s] * (cw, ch) + 0.5).astype(np.uint32).T\n",
    "    else:\n",
    "        v, u = (texcoords * (cw, ch) + 0.5).astype(np.uint32).T\n",
    "    # clip texcoords to image\n",
    "    np.clip(u, 0, ch-1, out=u)\n",
    "    np.clip(v, 0, cw-1, out=v)\n",
    "\n",
    "    # perform uv-mapping\n",
    "    out[i[m], j[m]] = color[u[m], v[m]]\n",
    "\n",
    "\n",
    "out = np.empty((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "while True:\n",
    "    # Grab camera data\n",
    "    if not state.paused:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        depth_frame = decimate.process(depth_frame)\n",
    "\n",
    "        # Grab new intrinsics (may be changed by decimation)\n",
    "        depth_intrinsics = rs.video_stream_profile(\n",
    "            depth_frame.profile).get_intrinsics()\n",
    "        w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        depth_colormap = np.asanyarray(\n",
    "            colorizer.colorize(depth_frame).get_data())\n",
    "\n",
    "        if state.color:\n",
    "            mapped_frame, color_source = color_frame, color_image\n",
    "        else:\n",
    "            mapped_frame, color_source = depth_frame, depth_colormap\n",
    "\n",
    "        points = pc.calculate(depth_frame)\n",
    "        pc.map_to(mapped_frame)\n",
    "\n",
    "        # Pointcloud data to arrays\n",
    "        v, t = points.get_vertices(), points.get_texture_coordinates()\n",
    "        verts = np.asanyarray(v).view(np.float32).reshape(-1, 3)  # xyz\n",
    "        texcoords = np.asanyarray(t).view(np.float32).reshape(-1, 2)  # uv\n",
    "\n",
    "    # Render\n",
    "    now = time.time()\n",
    "\n",
    "    out.fill(0)\n",
    "\n",
    "    grid(out, (0, 0.5, 1), size=1, n=10)\n",
    "    frustum(out, depth_intrinsics)\n",
    "    axes(out, view([0, 0, 0]), state.rotation, size=0.1, thickness=1)\n",
    "\n",
    "    if not state.scale or out.shape[:2] == (h, w):\n",
    "        pointcloud(out, verts, texcoords, color_source)\n",
    "    else:\n",
    "        tmp = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        pointcloud(tmp, verts, texcoords, color_source)\n",
    "        tmp = cv2.resize(\n",
    "            tmp, out.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n",
    "        np.putmask(out, tmp > 0, tmp)\n",
    "\n",
    "    if any(state.mouse_btns):\n",
    "        axes(out, view(state.pivot), state.rotation, thickness=4)\n",
    "\n",
    "    dt = time.time() - now\n",
    "\n",
    "    cv2.setWindowTitle(\n",
    "        state.WIN_NAME, \"RealSense (%dx%d) %dFPS (%.2fms) %s\" %\n",
    "        (w, h, 1.0/dt, dt*1000, \"PAUSED\" if state.paused else \"\"))\n",
    "\n",
    "    cv2.imshow(state.WIN_NAME, out)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord(\"r\"):\n",
    "        state.reset()\n",
    "\n",
    "    if key == ord(\"p\"):\n",
    "        state.paused ^= True\n",
    "\n",
    "    if key == ord(\"d\"):\n",
    "        state.decimate = (state.decimate + 1) % 3\n",
    "        decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)\n",
    "\n",
    "    if key == ord(\"z\"):\n",
    "        state.scale ^= True\n",
    "\n",
    "    if key == ord(\"c\"):\n",
    "        state.color ^= True\n",
    "\n",
    "    if key == ord(\"s\"):\n",
    "        cv2.imwrite('./out.png', out)\n",
    "\n",
    "    if key == ord(\"e\"):\n",
    "        points.export_to_ply('./out.ply', mapped_frame)\n",
    "\n",
    "    if key in (27, ord(\"q\")) or cv2.getWindowProperty(state.WIN_NAME, cv2.WND_PROP_AUTOSIZE) < 0:\n",
    "        break\n",
    "\n",
    "# Stop streaming\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Intel RealSense API",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "8ce51fb231350df207c05c2352339f5b129ccc325d9125005499071f447f3ecc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
